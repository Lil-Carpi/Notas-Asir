Internet profunda, web oscura, internet invisible o internet oculta es el contenido de internet que no esta indexado por los motores de busqueda convencionales, debido a diversos factores.

La principar causa de la existencia de la deep web es la imposibilidad de los motores de busqueda de encontrar o indexar gran parte de la informacion existente en Internet. Si los buscadores tuvieran esa capacidad, entonces la magnitud de la deep web se reduciria casi en su totalidad. No obstante , aunque los motores de busqueda pudieran indexar la informacion de la deep web esto no significaria que esta dejara de existir, ya que siempre existiran las paginas privadas. Los motores de busqueda no pueden acceder a la informacion de estas paginas y solo determinados usuarios, aquellos con contraseñas o codigos especiales pueden hacerlo.

Los motivos por los cuales los motores de busqueda no pueden indexar algunas paginas son:

- **Documentos o informacion oculta**: Archivos PDF que no se encuentran en las paginas web indexadas, listas de datos no publicas (sobre todo los ciber-criminales).
- **Web contextual**: Paginas cuyo contenido varia dependiendo del contexto (ej: Direccion IP del cliente, de las visitas anteriores, etc).
- **Contenido dinamico**: Paginas dinamicas obtenidas como respuesta a parametros, por ejemplo, datos enviados a traves de un formulario.
- **Contenido de acceso restringido**: Paginas protegidas con contraseña, contenido protegido por un Captcha, etc.
- **Contenido no HTML**: Contenido textual en archivos multimedia, otras extensiones como exe, rar, zip, etc.
- **Software**: Contenido oculto intencionadamente, que requiere un programa o protocolo especifico para poder acceder (ej: TOR, I2P, Freenet)
- **Paginas no enlazadas**: Paginas cuya existencia no tienen referencia los buscadores; por ejemplo, paginas que no tienen enlaces desde otras paginas.

En la Deep Web pueden establecerse contactos que no son monitoreados, nadie esta alli observando. Nada que se haga en esta zona puede ser asociado con la identidad de uno, a menos que uno lo desee. Ademas de las transferencias tanto de mercancia como de pagos son practicamente imposibles de rastrear.

La deep web no es una region prohibida o mistica en internet ni la tecnologia relacionada con ella es necesariamente conspirativa, peligrosa o ilegal. En ella se alberga todo tipo de recursos a los que es dificil de acceder mediante metodos comunes como motores de busqueda populares. Una parte de la deep web consiste en redes internas de instituciones cientificas y academicas que forman la denominada _Academic Invisible Web_, la cual se refiere a las bases de datos que contienen avances tecnologicos, publicaciones cientificas y material academico en general a los cuales no se puede acceder facilmente.

## ==Magnitud==

La deep web es un conjunto de sitios web y bases de datos que buscadores comunes no pueden encontrar ya que no estan indexadas. El contenido que se puede hallar dentro de la deep web es muy amplio.

El internet se ve dividido en 2 ramas, la internet profunda y la superficial. La internet superficial se compone de paginas indexadas en servidores DNS con una alfabetizacion y codificacion de pagina perfectamente entendible por los motores de busqueda, mientras que la internet profunda esta compuesta de paginas cuyos dominios estan registrados con extensiones .onion y los nombres de los dominios estan codificados en un trama hash. Estas paginas se sirven de firna ad hoc, y no necesitan registrarse, basta con que tu ordenador tenga funcionando un servicio onion, que hara las veces de un servicio dns, pero especial para las paginas de la deep web. Mediante una red P2P, se replican las bases de datos que contienen la resolucion de nombres hash.

Mientras que las paginas normales son identificadas mediante el protocolo UDP/IP, las paginas .onion son repetidas por el ramal al que se a conectado el navegador especializado para tal sistema de navegacion segura. El sistema de busqueda es le mismo que usa BitTorrent. El contenido se coloca en una base de datos y se proporciona solamente cuando lo solicite el usuario.

En 2010 se estimó que la informacion que se encuentra en la deep web es de 7500 terabytes, lo que es equivalente a aproximadamente 550 billones de documentos individuales.
En comparacion, se estima que la internet superficial contienen solo 19 terabytes de contenido y un billon de documentos individuales.

Tambien en 2010 se estimo que existian mas de 200 000 sitios en la deep web.

Estimaciones basadas en la extrapolacion de un estudio de la Universidad de California en Berkeley especula que actualmente la internet profunda debe tener unos 91 000 terabytes, o lo que es lo mismo, 6 674 Billones de documentos individuales.

# ==Metodos de profundizacion==

## Web Crawlers (Arañas)

Cuando se ingresa a un buscador y se realiza una consulta, el buscador no recorre la totalidad de internet en busca de posibles respuestas, sino que busca en su propia base de datos, que ha sido generada o indexada previamente. Se utiliza el termino _Web Crawler_ o robots inteligentes que van haciendo busquedas por enlaces de hipertexto de pagina en pagina, registrando la informacion ahi disponible.

	Cuando una persona realiza una consulta, el buscador no recorre la totalidad de internet en busca de las posibles respuestas, lo cual supondria una capacidad de reaccion bastante lenta. Lo que hace es buscar en su propia base de datos, que ha sido generada e indexada previamente. En sus labores de busqueda, indizacion y catalogacion, utilizan las llamadas Arañas (o robots inteligentes) que va saltando de una red a otra siguiendo los enlaces de hipertexto y registran la informacion alli disponible.

	[...] datos que se generan en tiempo real, como pueden ser valores en Bolsa, informacion del tiempo, horarios de trenes, etc.

El contenido que existe dentro de la deep web es en muy raras ocasiones mostrado como resultado en motoeres de busqueda convencionales, ya que las "arañas" no rastrean bases de datos ni los extraen. Las arañas no pueden tener acceso a paginas protegidas con contraseñas, alguinos desarrolladores que no desean que sus paginas sean encontradas insertan etiquetas especiales en el codigo para evitar que sea indexada. Las "arañas" son incapaces de mostrar paginas que no estan creadas en lenguaje HTML, ni tampoco puede leer enlaces que incluyen un signo de interrogacion. Pero ahora sitios web no creados con HTML o con signos de interrogacion estan siendo indexados por algunos motores de busqueda. Sin embargo, se calcula que incluso con estos buscadores mas avanzados solo se logra alcanzar el 16% de la informacion disponible en la deep web. Existen diferentes tecnicas de busqueda para extraer contenido de la internet profunda como librerias de bases de datos o simplemente conocer el URL al que quieres acceder y escribirlo manualmente.

## ==TOR==

The Onion Router (abreviado como TOR) es un proyecto diseñado e implementado por la marina de los Estados Unidos lanzado el 20 de septiembre del 2002. Posteriormente fue patrocinado por la EEF (Electronic Frontier Foundation, una organizacion en defensa de los derechos digitales). Al presente (2025), subsiste como The TOR Project, una organizacion sin animo de lucro galardonada en el 2011 por la Free Software Foundation por permitir que millones de personas en el mundo tengan libertad de acceso y expresion en internet manteniendo su privacidad y anonimato.

A diferencia de los navegadores de internet convencionales, TOR le permite a los usuarios navegar por la Web de forma anonima. Tor es descargado y 30 a 50 millones de veces al año, hay 0,8 millones de usuarios diarios de TOR y un incremento del 20% solamente en 2013. TOR puede acceder a unos 6 500 sitios web ocultos.

Cuando se ejecuta el software TOR, para acceder a la internet profunda, los datos de la computadora se cifran por capas. El software envia los datos a traves de una red de enlaces a otros equipos -Llamados en ingles "_relays_" ('nodos')- y lo va retransmitiendo quitando una capa antes de retransmitirlo de nuevo, esta trayectoria cambia con frecuencia. TOR cuenta con mas de 4 000 retransmisiones y todos los datos cifrados pasan  a traves de -por lo menos- tres de estos _relays_.
Una vez que la ultima capa de cifrado es retirado por un nodo de salida, se conecta a la pagina web que desea visitar.

El contenido puede ser encontrado dentro de la deep web es muy vasto, se encuentran por ejemplo, datos que se generan en tiempo real, como pueden ser valores de Bolsa, informacion del tiempo, horarios de trenes, bases de datos sobre agencias de inteligencia, disidentes politicos y contenidos criminales.